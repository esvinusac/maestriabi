{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingeniería de Características (Feature Engineering FE)\n",
    "+ Proceso para utilizar el conocimiento de la información o data que tenemos disponible para generar y modificar características nuevas.\n",
    "+ Muchos algoritmos de machine Learning trabajan en un espacio geométrico, es decir los algoritmos están diseñados para trabajar con distancias, aglomeraciones, etc.\n",
    "+ Muchos algoritmos de ML utilizan la optimización para resolver un problema excepto los basados en aglomeraciones.\n",
    "+ Ingeniería de Características (FE) es el pase entre la data real (la que viene del negocio ya sea a través de una base de datos relacional/data warehouse, etc.) y el espacio geométrico en el que queremos encontrar ese algoritmo que nos va a servir para resolver nuestro problema, una regresión, una clasificación, etc.\n",
    "    - Si tenemos una tabla con registros de clientes, entonces cada cliente se va a representar como un punto geométrico, sobre el que el algoritmo si puede actuar\n",
    "    - En FE podemos producir diferentes versiones del espacio geométrico, dependiendo del tipo de operación y del tipo de decisión que tome al momento de hacer el proceso de FE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas, Task o Pipeline en la ingeniería de Características\n",
    "1. Imputación de datos faltantes\n",
    "2. Codificación de variables categóricas\n",
    "3. Transformación de Variables\n",
    "4. Generación de nuevas características (mutación de datos)\n",
    "5. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imputación de datos faltantes\n",
    "+ A veces en un dataset puede suceder que algunos valores no estén disponibles por diversas razones:\n",
    "    - Por perdida\n",
    "    - Por Inexistencia\n",
    "    - Por no disponibilidad\n",
    "+ La falta de datos suele ser uno de los retos mas importantes y comunes al momento de generar un modelo de ML\n",
    "+ La falta de datos puede generar efectos negativos en la capacidad predictiva de un modelo\n",
    "    - Esto porque la distribución de probabilidad se concibe solo considerando la data que si esta disponible\n",
    "+ Tres Mecanismos principales de imputación de datos (y que representan la naturaleza del dato faltante):\n",
    "    - MCAR (Missing Completely at Random)\n",
    "        * La probabilidad que falte un valor es la misma para cualquier observación\n",
    "        * No existe ninguna relación entre las demás observaciones, tanto faltantes como existentes\n",
    "    - MAR (Missing at Random)\n",
    "        * Se genera cuando existe una relación entre los faltantes y algunas variables disponibles del dataset\n",
    "        * En el ejemplo de la clase (tipo de sangre, peso), vemos que la proporción de faltantes en peso para tipo de sangre A es la misma para los de tipo de sangre B\n",
    "    - MNAR (Missing not at Random)\n",
    "        * Se presenta cuando existe un mecanismo que genere intrínsecamente la falta de los valores\n",
    "        * En el ejemplo de la clase (obesidad, días de ejercicio, depresión), si observamos la proporción de los SI contra los NO, puede ser que la persona que presentaba obesidad no quiso decirnos cuantos días de ejercicio hacía.  Parece ser que hay una relación natural entre los Días de ejercicio y la Obesidad.  De la misma manera ocurre con la Depresión.\n",
    "+ El Data imputation se refiere al tratamiento de datos faltantes\n",
    "+ La imputación de datos se refiere a la acción de reemplazar los valores faltantes de un conjunto de datos, con una estimación del posible valor real\n",
    "+ La idea principal es proveerle un dataset con la mayor cantidad de información posible a un algoritmo de ML\n",
    "    - Si hay datos faltantes, es como que si hubiera algún vacío (algún hoyo), entonces cuando el algoritmo pase por allí, no va a saber que hacer\n",
    "    - Para que todo esto funcione, debemos trabajar con un conjunto convexo.  Es decir, un espacio donde no hay data inexistente. Donde se puede trazar una línea recta desde un punto A a un punto B\n",
    "+ Métodos de Data Imputation\n",
    "    - Para datos numéricos\n",
    "        * Imputación de media y mediana\n",
    "            + Según lo visto en clase, antes de realizar la imputación se debe verificar que las columnas del dataset tienen una proporción de faltantes menor al 5% o a un rate especificado\n",
    "        * Imputación de valores arbitrarios         \n",
    "            + Se trata de seleccionar un valor arbitrariamente.  Aleatorio es que se genera de forma computacional por medio de una distribución de probabilidad, mientras que arbitrariamente significa que yo decido que valor es. La idea es identificar cual es el valor más adecuado (arbitrariamente).  Hay varios métodos:\n",
    "                - Un valor arbitrario puede ser un numero un poco mas grande que el valor mas grande en la columna a imputar \n",
    "                - Un valor arbitrario puede ser el valor de corrimiento de la distribución menos uno\n",
    "        * Imputación probabilística\n",
    "    - Para variables categóricas\n",
    "        * Imputación por frecuencia\n",
    "        * Agregar categoría de faltante (no es el NA, si no es crear una nueva categoría dentro de la data)        \n",
    "    - Para variables mixtas (por ejemplo, el número de ticket, tiene más de un tipos de datos)\n",
    "        * Complete Case Analysis (CCA)\n",
    "            + Básicamente es eliminar los que tienen NAs\n",
    "            + Se refiere a remover todas las observaciones que poseen faltantes en cualquier variable del dataset\n",
    "            + De este modo se utiliza la información que se considera “completa” dentro del dataset\n",
    "            + Aplicable a datos numéricos, categóricos, y mixtos\n",
    "            + Según lo visto en clase, se recomienda usar cuando la cantidad de datos faltantes <= 5%\n",
    "            + Ventajas:\n",
    "                - Es simple\n",
    "                - No requiere ninguna manipulación interna de los datos\n",
    "                - Prevalecen las propiedades probabilísticas de los datos, no modificamos la distribución de los datos\n",
    "            + Desventajas:\n",
    "                - Puede excluir a una gran cantidad de datos del dataset original\n",
    "                - Omite observaciones que podrían ser particularmente importantes para la construcción del modelo\n",
    "                - Podría generarse un dataset sesgado debido a que se pueden omitir observaciones que contengan categorías especificas\n",
    "                - En producción el modelo podría producir errores ya que, si aparece una observación con alguna categoría eliminada, el predictor no sabrá como tratarla                \n",
    "        * Indicador de faltantes\n",
    "        * Imputación por muestra aleatoria        \n",
    "+ Según lo visto en clase, como regla para determinar cuando hacer la imputación: \n",
    "    - Si la columna tiene <=  20% de faltantes entonces si podemos trabajarla, si esta entre el 20% y  50% hay que tener cuidado, mayor de 50% recomienda tirar esa columna y utilizar otra (Siempre depende de cada situación)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Codificación de variables categóricas\n",
    "+ Consiste en transformar las variables categóricas en una representación numérica para que se conviertan en parte del espacio geométrico donde se va a resolver el problema\n",
    "+ Ejemplos de métodos:\n",
    "    - One hot encoding\n",
    "        * Crea una columna por cada categoría disponible \n",
    "        * Desventaja:\n",
    "            + Para variables con muchas clases, se crean muchas columnas nuevas\n",
    "        * Si hay más de 10 categorías, no es conveniente utilizar este método.\n",
    "    - Frequency encoding\n",
    "        * Caso especial: Density Encoding\n",
    "    - Verificación de impacto de la variable\n",
    "        * Hacemos un gráfico de BloxPlot de la variable vs el target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transformación de Variables (Numéricas)\n",
    "\n",
    "+ A veces pasa, que hay variables (numéricas) que tiene algún tipo de sesgo (no tienen una distribución normal).  Entonces lo que busca la transformación es hacer la mejor representación posible de esa columna según lo que espera cada algoritmo del comportamiento de cada columna\n",
    "+ Por ejemplo, el algoritmo LDA, asume que la distribución de probabilidad de cada uno de los grupos donde se hace la separación es normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generación de nuevas características (mutación de datos)\n",
    "+ También se llama data aumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Scaling\n",
    "+ Se hace debido a las diferentes escalas entre las variables y el outcome (Conocido como el problema de escala pobre), de tal manera que todas las columnas estén en el mismo orden de magnitud (ya sea en decenas, millardos, etc), esto para que la estructura donde se resuelve el problema este equilibrada, y así el algoritmo se le facilita mas donde encontrar y resolver donde está el objetivo\n",
    "+ Por ejemplo, tenemos la edad de una persona (que esta en el rango de 18 a 90) y otra columna del monto de la transacción (que están en una escala de miles), es decir la escala es completamente distinta, uno es muy grande y otro muy pequeño.  El problema es que el área donde se debe resolver el problema gráficamente parece un ovalo (y no un círculo) entonces El problema es que el algoritmo de ML empieza a rebotar, ya que cada cambio de dirección es muy pegado (por que la escala es diferente) , y puede ser que eventualmente no resuelva el problema o puede ser que se tarde un montón en encontrar la solución.  Entonces el FE lo que hace es normalizar la data o que este equilibrada, es decir que todas las columnas estén en el mismo orden de magnitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
